---
title: "STAT/MATH 495: Problem Set 04"
author: "MERON GEDRAGO"
date: "2017-10-03"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE)
set.seed(76)
```

# Collaboration

Please indicate who you collaborated with on this assignment:


# Load packages, data, model formulas

```{r, warning=FALSE,echo=FALSE}
library(tidyverse)
credit <- read_csv("http://www-bcf.usc.edu/~gareth/ISL/Credit.csv") %>%
  select(-X1) %>%
  mutate(ID = 1:n()) %>% 
  select(ID, Balance, Income, Limit, Rating, Age, Cards, Education)
```

You will train the following 7 models on `credit_train`...

```{r}
model1_formula <- as.formula("Balance ~ 1")
model2_formula <- as.formula("Balance ~ Income")
model3_formula <- as.formula("Balance ~ Income + Limit")
model4_formula <- as.formula("Balance ~ Income + Limit + Rating")
model5_formula <- as.formula("Balance ~ Income + Limit + Rating + Age")
model6_formula <- as.formula("Balance ~ Income + Limit + Rating + Age + Cards")
model7_formula <- as.formula("Balance ~ Income + Limit + Rating + Age + Cards + Education")
```

.. where `credit_train` is defined below, along with `credit_test`.

```{r}
set.seed(79)
#sample 20 rows of the credit dataset to the training dataset, while the 380 rows are randomly picked from the credit dataset to the test dataset
credit_train <- credit %>% 
  sample_n(20)
credit_test <- credit %>% 
  anti_join(credit_train, by="ID")
```


# RMSE vs number of coefficients

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Placeholder vectors of length 7. For now, I've filled them with arbitrary 
# values; you will fill these in
RMSE_train <- runif(n=7)
RMSE_test <- runif(n=7)
# Do your work here:
#create a function that inputs the modelformula and model number to give back the RMSE of each model 
ModelRMSE <- function(formulainput, ModelNum) {
  model_lm <- lm(formulainput, data=credit_train)
model_lm %>% 
  broom::tidy(conf.int=TRUE)
predictions <- model_lm %>% 
  broom :: augment(newdata=credit_train)
MSE <- mean((predictions$Balance - predictions$.fitted)^2)
MSE
RMSE_train[ModelNum] <- sqrt(MSE)
RMSE_train[ModelNum]
}
# find the RMSE_train, 

RMSE_train[1] <-ModelRMSE(model1_formula,1) 
RMSE_train[2] <- ModelRMSE(model2_formula,2)
RMSE_train[3] <- ModelRMSE(model3_formula,3) 
RMSE_train[4] <- ModelRMSE(model4_formula,4)  
RMSE_train[5] <- ModelRMSE(model5_formula,5) 
RMSE_train[6] <- ModelRMSE(model6_formula,6) 
RMSE_train[7] <- ModelRMSE(model7_formula,7)


#find the RMSE test for model 

ModelRMSE <- function(formulainput, ModelNum) {
  model_lm <- lm(formulainput, data=credit_train)
model_lm %>% 
  broom::tidy(conf.int=TRUE)
predictions <- model_lm %>% 
  broom :: augment(newdata=credit_test)
 
MSE <- mean((predictions$Balance - predictions$.fitted)^2)
RMSE_test[ModelNum] <- sqrt(MSE)
RMSE_test[ModelNum]
}
RMSE_test[1] <-ModelRMSE(model1_formula,1) 
RMSE_test[2] <- ModelRMSE(model2_formula,2)
RMSE_test[3] <- ModelRMSE(model3_formula,3) 
RMSE_test[4] <- ModelRMSE(model4_formula,4)  
RMSE_test[5] <- ModelRMSE(model5_formula,5) 
RMSE_test[6] <- ModelRMSE(model6_formula,6) 
RMSE_test[7] <- ModelRMSE(model7_formula,7)

# Save results in a data frame. Note this data frame is in wide format.
results <- data_frame(
  num_coefficients = 1:7,
  RMSE_train,
  RMSE_test
) 
# Some cleaning of results
results <- results %>% 
  # More intuitive names:
  rename(
    `Training data` = RMSE_train,
    `Test data` = RMSE_test
  ) %>% 
  # Convert results data frame to "tidy" data format i.e. long format, so that we
  # can ggplot it
  gather(type, RMSE, -num_coefficients)


ggplot(results, aes(x=num_coefficients, y=RMSE, col=type)) +
  geom_line() + 
  labs(x="# of coefficients", y="RMSE", col="Data used to evaluate \nperformance of fitted model")
```


# Interpret the graph

Compare and contrast the two curves and hypothesize as to the root cause of any differences.

From the above graph, the blue curve measures the predictive strength of the training set on itself using RMSE,  this can be referred to as 'Texas Sharpshooter Fallacy'. Since we are increasing the # of coefficient (i.e also the degrees of the freedom), we see that as the the number of coefficients increases , the RMSE keeps decreasing regardless of the fact that we are overfitting. This is exactly what we would expect because we predicting the same data we used to make the model. 


Whereas, the red line depicts  a level of cross validation, it shows that the RMSE is relatively high at the two ends of x axis. The red line shows that that RMSE is high when the data is underfitted and also when it is overfitted. 


So the key difference between the two lines is the dataset we use to measure our predictive strength.This results in the red line indicating the overfitting and underfitting of the data. 








# Bonus 

Repeat the whole process, but let `credit_train` be a random sample of size 380
from `credit` instead of 20. Now compare and contrast this graph with the
one above and hypothesize as to the root cause of any differences.

```{r}
set.seed(79)
credit_train <- credit %>% 
  sample_n(380)
credit_test <- credit %>% 
  anti_join(credit_train, by="ID")
```


# RMSE vs number of coefficients

```{r,  echo=FALSE, warning=FALSE, message=FALSE}
# Placeholder vectors of length 7. For now, I've filled them with arbitrary 
# values; you will fill these in
RMSE_train <- runif(n=7)
RMSE_test <- runif(n=7)


# Do your work here:
#find the RMSE train  for model 
ModelRMSE <- function(formulainput, ModelNum) {
  model_lm <- lm(formulainput, data=credit_train)
model_lm %>% 
  broom::tidy(conf.int=TRUE)
predictions <- model_lm %>% 
  broom :: augment(newdata=credit_train)
 
MSE <- mean((predictions$Balance - predictions$.fitted)^2)
MSE
RMSE_train[ModelNum] <- sqrt(MSE)
RMSE_train[ModelNum]
}

RMSE_train[1] <-ModelRMSE(model1_formula,1) 
RMSE_train[2] <- ModelRMSE(model2_formula,2)
RMSE_train[3] <- ModelRMSE(model3_formula,3) 
RMSE_train[4] <- ModelRMSE(model4_formula,4)  
RMSE_train[5] <- ModelRMSE(model5_formula,5) 
RMSE_train[6] <- ModelRMSE(model6_formula,6) 
RMSE_train[7] <- ModelRMSE(model7_formula,7)

#find the RMSE test for model 

ModelRMSE <- function(formulainput, ModelNum) {
  model_lm <- lm(formulainput, data=credit_train)
model_lm %>% 
  broom::tidy(conf.int=TRUE)
predictions <- model_lm %>% 
  broom :: augment(newdata=credit_test)
 
MSE <- mean((predictions$Balance - predictions$.fitted)^2)
MSE
RMSE_test[ModelNum] <- sqrt(MSE)
RMSE_test[ModelNum]
}
RMSE_test[1] <-ModelRMSE(model1_formula,1) 
RMSE_test[2] <- ModelRMSE(model2_formula,2)
RMSE_test[3] <- ModelRMSE(model3_formula,3) 
RMSE_test[4] <- ModelRMSE(model4_formula,4)  
RMSE_test[5] <- ModelRMSE(model5_formula,5) 
RMSE_test[6] <- ModelRMSE(model6_formula,6) 
RMSE_test[7] <- ModelRMSE(model7_formula,7)

# Save results in a data frame. Note this data frame is in wide format.
results <- data_frame(
  num_coefficients = 1:7,
  RMSE_train,
  RMSE_test
) 
# Some cleaning of results
results <- results %>% 
  # More intuitive names:
  rename(
    `Training data` = RMSE_train,
    `Test data` = RMSE_test
  ) %>% 
  # Convert results data frame to "tidy" data format i.e. long format, so that we
  # can ggplot it
  gather(type, RMSE, -num_coefficients)

ggplot(results, aes(x=num_coefficients, y=RMSE, col=type)) +
  geom_line() + 
  labs(x="# of coefficients", y="RMSE", col="Data used to evaluate \nperformance of fitted model")
```
# Interpret the graph

Compare and contrast the two curves and hypothesize as to the root cause of any differences.

We see some similarities and differences between the above graph and the graph from part1. 
The similarity is that we see a constantly decreasing pattern of the blue line, and we also see a dip of the red line. Major difference is that the red line doesn't have a more dramatic 'fall and rise' pattern as we had in the first graph.

This difference is caused by the sampling in which our training set was only 5% of our data set while the rest was a test set.And due to the small number of training set, we weren't able to obtain a model that was able to capture enough information on the our dataset to make a prediction of the test data.The model doesn't even have enough information that we cant clearly much difference when the test set is being under or overfitted.  

